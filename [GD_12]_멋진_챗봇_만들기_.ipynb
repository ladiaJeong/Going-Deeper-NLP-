{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ladiaJeong/Going-Deeper-NLP-/blob/master/%5BGD_12%5D_%EB%A9%8B%EC%A7%84_%EC%B1%97%EB%B4%87_%EB%A7%8C%EB%93%A4%EA%B8%B0_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5df1124b",
      "metadata": {
        "id": "5df1124b",
        "outputId": "61c2f46b-a91a-40d0-dc0d-56aa80be4a7f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.21.4\n",
            "1.3.3\n",
            "2.6.0\n",
            "3.6.5\n",
            "3.8.3\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import gensim\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import sentencepiece as spm\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from nltk.translate.bleu_score import SmoothingFunction\n",
        "\n",
        "import gensim\n",
        "\n",
        "import re\n",
        "import os\n",
        "import random\n",
        "import math\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(np.__version__)\n",
        "print(pd.__version__)\n",
        "print(tf.__version__)\n",
        "print(nltk.__version__)\n",
        "print(gensim.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4abc19c",
      "metadata": {
        "id": "f4abc19c"
      },
      "source": [
        "#### Step 1. 데이터 다운로드"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f14eebef",
      "metadata": {
        "id": "f14eebef",
        "outputId": "481e9349-91e0-4da4-be20-02e0c57ee98b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-02-08 09:16:44--  https://raw.githubusercontent.com/songys/Chatbot_data/master/ChatbotData.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 889842 (869K) [text/plain]\n",
            "Saving to: ‘ChatbotData.csv.5’\n",
            "\n",
            "ChatbotData.csv.5   100%[===================>] 868.99K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2023-02-08 09:16:44 (24.6 MB/s) - ‘ChatbotData.csv.5’ saved [889842/889842]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#https://github.com/songys/Chatbot_data\n",
        "!wget https://raw.githubusercontent.com/songys/Chatbot_data/master/ChatbotData.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6db5e777",
      "metadata": {
        "id": "6db5e777"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('ChatbotData.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46e44952",
      "metadata": {
        "id": "46e44952",
        "outputId": "ef7c11e0-8c1c-48bc-b32d-df8c850db7c7"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Q</th>\n",
              "      <th>A</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>12시 땡!</td>\n",
              "      <td>하루가 또 가네요.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1지망 학교 떨어졌어</td>\n",
              "      <td>위로해 드립니다.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3박4일 놀러가고 싶다</td>\n",
              "      <td>여행은 언제나 좋죠.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3박4일 정도 놀러가고 싶다</td>\n",
              "      <td>여행은 언제나 좋죠.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>PPL 심하네</td>\n",
              "      <td>눈살이 찌푸려지죠.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 Q            A  label\n",
              "0           12시 땡!   하루가 또 가네요.      0\n",
              "1      1지망 학교 떨어졌어    위로해 드립니다.      0\n",
              "2     3박4일 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
              "3  3박4일 정도 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
              "4          PPL 심하네   눈살이 찌푸려지죠.      0"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df. head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b0b6f0c",
      "metadata": {
        "id": "4b0b6f0c",
        "outputId": "698c487e-e7a9-416c-f448-a70ac9548aeb"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Q</th>\n",
              "      <th>A</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>11823</td>\n",
              "      <td>11823</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>11662</td>\n",
              "      <td>7779</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>외로워</td>\n",
              "      <td>맛있게 드세요.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>4</td>\n",
              "      <td>22</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            Q         A\n",
              "count   11823     11823\n",
              "unique  11662      7779\n",
              "top       외로워  맛있게 드세요.\n",
              "freq        4        22"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.describe(include = 'O')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34ce364d",
      "metadata": {
        "id": "34ce364d"
      },
      "source": [
        "#### Step 2. 데이터 정제"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "233e546e",
      "metadata": {
        "id": "233e546e",
        "outputId": "8b63e1b7-a23e-406b-b442-8bb5ecdddced"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "슝=3\n"
          ]
        }
      ],
      "source": [
        "def preprocess_sentence(sentence):\n",
        "    sentence = sentence.lower() #소문자\n",
        "    sentence = re.sub(r'[\" \"]+', \" \", sentence) #정규 표현식과 일치하는 부분에 대해서 다른 문자열로 대체합니다.\n",
        "    sentence = re.sub(r\"[^ㄱ-ㅎ가-힣a-zA-Z!.,?0-9]+\", \" \", sentence)\n",
        "    sentence = sentence.strip() #문자열을 분리하여 리스트로 리턴합니다.\n",
        "    return sentence\n",
        "\n",
        "print(\"슝=3\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15a0dcaa",
      "metadata": {
        "id": "15a0dcaa"
      },
      "source": [
        "#### Step 3. 데이터 토큰화"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e613c35b",
      "metadata": {
        "id": "e613c35b"
      },
      "outputs": [],
      "source": [
        "from konlpy.tag import Mecab\n",
        "\n",
        "mecab = Mecab()\n",
        "\n",
        "questions = df['Q'].values\n",
        "answers = df['A'].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b080941",
      "metadata": {
        "id": "9b080941"
      },
      "outputs": [],
      "source": [
        "def build_corpus(src, tgt, tokenizer, max_len= 50) : #소스 문장 데이터와 타겟 문장 데이터를 입력으로 받습니다.\n",
        "    \n",
        "    temp_corpus = list(set(src + '\\t' + tgt)) #데이터를 앞서 정의한 preprocess_sentence() 함수로 정제하고, 토큰화합니다.\n",
        "    \n",
        "    src_corpus, tgt_corpus = list(), list()\n",
        "    \n",
        "    for sen in temp_corpus :\n",
        "        src_sen, tgt_sen = sen.split('\\t')\n",
        "        \n",
        "        #src\n",
        "        src_sen = preprocess_sentence(src_sen) # 전처리\n",
        "        src_sen = tokenizer.morphs(src_sen) # 형태소 토크나이\n",
        "\n",
        "        tgt_sen = preprocess_sentence(tgt_sen)\n",
        "        tgt_sen = tokenizer.morphs(tgt_sen)\n",
        "        \n",
        "        if len(src_sen) < max_len and len(tgt_sen) < max_len :\n",
        "            src_corpus.append(' '.join(src_sen))\n",
        "            tgt_corpus.append(' '.join(tgt_sen))\n",
        "        \n",
        "  \n",
        "    \n",
        "    return src_corpus, tgt_corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7753d5fa",
      "metadata": {
        "id": "7753d5fa",
        "outputId": "0d77c96b-a1ef-4917-ff15-fcdd1f7f5a23"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "11750\n",
            "11750\n"
          ]
        }
      ],
      "source": [
        "\n",
        "question_corpus, answer_corpus = build_corpus(questions, answers, mecab, max_len = 50)\n",
        "#토큰의 개수가 일정 길이 이상인 문장은 데이터에서 제외합니다.\n",
        "print(len(question_corpus))\n",
        "print(len(answer_corpus ))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "556a006e",
      "metadata": {
        "id": "556a006e"
      },
      "source": [
        "#### Step 4. Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff648737",
      "metadata": {
        "id": "ff648737"
      },
      "outputs": [],
      "source": [
        "from gensim.models import KeyedVectors, Word2Vec\n",
        "from gensim.test.utils import datapath"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9084698",
      "metadata": {
        "id": "d9084698"
      },
      "outputs": [],
      "source": [
        "w2vpath = os.getenv('HOME') + \"/aiffel/gd-12/ko.bin\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9cacef22",
      "metadata": {
        "id": "9cacef22",
        "outputId": "1c495f53-54b8-445b-976c-517237618f0a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gensim==3.8.3 in /opt/conda/lib/python3.9/site-packages (3.8.3)\n",
            "Requirement already satisfied: six>=1.5.0 in /opt/conda/lib/python3.9/site-packages (from gensim==3.8.3) (1.16.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /opt/conda/lib/python3.9/site-packages (from gensim==3.8.3) (1.7.1)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /opt/conda/lib/python3.9/site-packages (from gensim==3.8.3) (1.21.4)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /opt/conda/lib/python3.9/site-packages (from gensim==3.8.3) (5.2.1)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade gensim==3.8.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ef517f3",
      "metadata": {
        "id": "1ef517f3",
        "outputId": "aaac9234-8c88-4187-b115-f34594dad994"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3.8.3\n"
          ]
        }
      ],
      "source": [
        "print(gensim.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d7f06ba",
      "metadata": {
        "id": "6d7f06ba"
      },
      "outputs": [],
      "source": [
        "wv = Word2Vec.load(w2vpath) # gensim version 4.0 이하로 다시 설치함"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a15ed428",
      "metadata": {
        "scrolled": true,
        "id": "a15ed428",
        "outputId": "6d947316-e147-450a-e811-e4f86215b518"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_183/1528586178.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
            "  wv.most_similar(\"사람\")\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[('젊은이', 0.6494427919387817),\n",
              " ('여인', 0.6287257671356201),\n",
              " ('백성', 0.6063710451126099),\n",
              " ('포졸', 0.6043275594711304),\n",
              " ('죄인', 0.5960500836372375),\n",
              " ('선비', 0.5868039131164551),\n",
              " ('부녀자', 0.5654411315917969),\n",
              " ('죄수', 0.5639811754226685),\n",
              " ('구경꾼', 0.5620019435882568),\n",
              " ('손님', 0.5589558482170105)]"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wv.most_similar(\"사람\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "735f01d3",
      "metadata": {
        "id": "735f01d3",
        "outputId": "0e2d5dec-9b37-40f0-a313-fa6e32a0f4ab"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_183/2324208733.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
            "  wv.most_similar(\"바나나\")\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[('코코넛', 0.8097119927406311),\n",
              " ('시금치', 0.7701147794723511),\n",
              " ('레몬', 0.76884925365448),\n",
              " ('땅콩', 0.7684735059738159),\n",
              " ('파인애플', 0.7639915347099304),\n",
              " ('녹차', 0.7631460428237915),\n",
              " ('딸기', 0.7617197036743164),\n",
              " ('바닐라', 0.7497864961624146),\n",
              " ('파슬리', 0.7447543144226074),\n",
              " ('코코아', 0.7408244609832764)]"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wv.most_similar(\"바나나\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f5ca5c2",
      "metadata": {
        "id": "8f5ca5c2"
      },
      "outputs": [],
      "source": [
        "# Lexical Substitution 구현하기\n",
        "def lexical_sub(sentence, word2vec):\n",
        "    res = \"\"\n",
        "    toks = sentence.split()\n",
        "\n",
        "    try:\n",
        "        _from = random.choice(toks)\n",
        "        _to = word2vec.most_similar(_from)[0][0]\n",
        "        \n",
        "    except:   # 단어장에 없는 단어\n",
        "        return None\n",
        "\n",
        "    for tok in toks:\n",
        "        if tok is _from: res += _to + \" \"\n",
        "        else: res += tok + \" \"\n",
        "\n",
        "    return res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6082019",
      "metadata": {
        "id": "c6082019",
        "outputId": "044f1a99-fcce-4097-bfd7-e67d28451d05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "None\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_183/3779415679.py:8: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
            "  _to = word2vec.most_similar(_from)[0][0]\n"
          ]
        }
      ],
      "source": [
        "sentence = '나는 착한 사람 입니다'\n",
        "res = lexical_sub(sentence, wv)\n",
        "print(res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "074f2622",
      "metadata": {
        "id": "074f2622",
        "outputId": "bae22111-b87b-45ba-daa9-c24b6122d84c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "너희는 세상의 빛 이라고 \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_183/3779415679.py:8: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
            "  _to = word2vec.most_similar(_from)[0][0]\n"
          ]
        }
      ],
      "source": [
        "sentence = '너희는 세상의 빛 이라'\n",
        "res = lexical_sub(sentence, wv)\n",
        "print(res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4dadf109",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "5c1740c500484af7b30293312cd99136"
          ]
        },
        "id": "4dadf109",
        "outputId": "4ce50fe0-799f-4fb5-eb33-b14993735693"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5c1740c500484af7b30293312cd99136",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "이성 으로 는 감당 할 수 없 을 것 같 아\n",
            "잠깐 눈 을 붙이 고 마음 을 가라앉혀 보 세요 .\n",
            "사랑 없 는 삶 은 불행 할까 ?\n",
            "삶 곳곳 에 사랑 이 숨 쉬 고 있 어요 .\n"
          ]
        }
      ],
      "source": [
        "for old_src,old_tgt in tqdm(zip(question_corpus[:2], answer_corpus[:2])):\n",
        "    print(old_src)\n",
        "    print(old_tgt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29e41108",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "06927ad8a449412f80168b4a38fc6151",
            "983ddb240e064254b82d6a54e2e7cdfe"
          ]
        },
        "id": "29e41108",
        "outputId": "d05163ee-bf42-4213-981a-fe06bd8962a3"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "06927ad8a449412f80168b4a38fc6151",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_183/3779415679.py:8: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
            "  _to = word2vec.most_similar(_from)[0][0]\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "983ddb240e064254b82d6a54e2e7cdfe",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "new_corpus = []\n",
        "\n",
        "for old_src, old_tgt in tqdm(zip(question_corpus, answer_corpus)):\n",
        "    new_src = lexical_sub(old_src, wv)\n",
        "    if new_src is not None: \n",
        "        new_corpus.append(new_src +'\\t' +old_tgt)\n",
        "    # Augmentation이 없더라도 원본 문장을 포함시킵니다\n",
        "    new_corpus.append(old_src + '\\t' + old_tgt)\n",
        "\n",
        "for old_tgt, old_src in tqdm(zip(answer_corpus,question_corpus)):\n",
        "    new_tgt = lexical_sub(old_tgt, wv)\n",
        "    if new_tgt is not None: \n",
        "        new_corpus.append(old_src + '\\t' + new_tgt)\n",
        "    # Augmentation이 없더라도 원본 문장을 포함시킵니다\n",
        "    new_corpus.append(old_src + '\\t' + old_tgt)\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2dd14934",
      "metadata": {
        "id": "2dd14934"
      },
      "outputs": [],
      "source": [
        "new_corpus = list(set(new_corpus)) #공백제거"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df8e61a9",
      "metadata": {
        "id": "df8e61a9"
      },
      "source": [
        "#### Step 5. 데이터 벡터화"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08f2d8bd",
      "metadata": {
        "id": "08f2d8bd",
        "outputId": "f4c03c24-7701-41be-f63e-5664f748cbdd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<start> 저 랑 한잔 해요 . <end>\n"
          ]
        }
      ],
      "source": [
        "q_corpus = []\n",
        "a_corpus = []\n",
        "\n",
        "for i in new_corpus : \n",
        "    q,a = i. split('\\t') \n",
        "    q_corpus.append(q)\n",
        "    a_corpus.append(a)\n",
        "    \n",
        "\n",
        "a_corpus = ['<start> ' + i + ' <end>' for i in a_corpus]\n",
        "print(a_corpus[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74bf6f9e",
      "metadata": {
        "id": "74bf6f9e"
      },
      "outputs": [],
      "source": [
        "test_size = int(len(a_corpus) * 0.05)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "887c0d65",
      "metadata": {
        "id": "887c0d65"
      },
      "outputs": [],
      "source": [
        "train_q = q_corpus[:-test_size]\n",
        "test_q = q_corpus[-test_size:]\n",
        "train_a = a_corpus[:-test_size]\n",
        "test_a = a_corpus[-test_size:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "deae6f18",
      "metadata": {
        "id": "deae6f18",
        "outputId": "9c3656e2-6a61-4f60-dbea-5593d3962963"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1607"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(test_q)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6c31a2b",
      "metadata": {
        "id": "c6c31a2b"
      },
      "outputs": [],
      "source": [
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "tokenizer.fit_on_texts(q_corpus + a_corpus)\n",
        "\n",
        "enc_tensor = tokenizer.texts_to_sequences(train_q)\n",
        "dec_tensor = tokenizer.texts_to_sequences(train_a)\n",
        "\n",
        "enc_tensor =  tf.keras.preprocessing.sequence.pad_sequences(enc_tensor, maxlen=50, padding='post')\n",
        "dec_tensor =  tf.keras.preprocessing.sequence.pad_sequences(dec_tensor, maxlen=50, padding='post')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e814b675",
      "metadata": {
        "id": "e814b675"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 64\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((enc_tensor, dec_tensor)).batch(batch_size=BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ecbdb679",
      "metadata": {
        "id": "ecbdb679"
      },
      "source": [
        "#### Step 6. 훈련하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7842dc88",
      "metadata": {
        "id": "7842dc88"
      },
      "outputs": [],
      "source": [
        "# Positional Encoding 구현\n",
        "\n",
        "def positional_encoding(pos, d_model):\n",
        "    def cal_angle(position, i):\n",
        "        return position / np.power(10000, (2*(i//2)) / np.float32(d_model))\n",
        "\n",
        "    def get_posi_angle_vec(position):\n",
        "        return [cal_angle(position, i) for i in range(d_model)]\n",
        "\n",
        "    sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(pos)])\n",
        "\n",
        "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])\n",
        "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])\n",
        "\n",
        "    return sinusoid_table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "829c5cae",
      "metadata": {
        "id": "829c5cae"
      },
      "outputs": [],
      "source": [
        "# 마스크 생성\n",
        "def generate_padding_mask(seq):\n",
        "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
        "\n",
        "def generate_lookahead_mask(size):\n",
        "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "    return mask\n",
        "\n",
        "def generate_masks(src, tgt):\n",
        "    enc_mask = generate_padding_mask(src)\n",
        "    dec_enc_mask = generate_padding_mask(src)\n",
        "\n",
        "    dec_lookahead_mask = generate_lookahead_mask(tgt.shape[1])\n",
        "    dec_tgt_padding_mask = generate_padding_mask(tgt)\n",
        "    dec_mask = tf.maximum(dec_tgt_padding_mask, dec_lookahead_mask)\n",
        "\n",
        "    return enc_mask, dec_enc_mask, dec_mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b6cd7de",
      "metadata": {
        "id": "1b6cd7de"
      },
      "outputs": [],
      "source": [
        "# Multi Head Attention 구현\n",
        "\n",
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "        \n",
        "        self.depth = d_model // self.num_heads\n",
        "        \n",
        "        self.W_q = tf.keras.layers.Dense(d_model)\n",
        "        self.W_k = tf.keras.layers.Dense(d_model)\n",
        "        self.W_v = tf.keras.layers.Dense(d_model)\n",
        "        \n",
        "        self.linear = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "    def scaled_dot_product_attention(self, Q, K, V, mask):\n",
        "        d_k = tf.cast(K.shape[-1], tf.float32)\n",
        "        QK = tf.matmul(Q, K, transpose_b=True)\n",
        "\n",
        "        scaled_qk = QK / tf.math.sqrt(d_k)\n",
        "\n",
        "        if mask is not None: scaled_qk += (mask * -1e9)  \n",
        "\n",
        "        attentions = tf.nn.softmax(scaled_qk, axis=-1)\n",
        "        out = tf.matmul(attentions, V)\n",
        "\n",
        "        return out, attentions\n",
        "        \n",
        "\n",
        "    def split_heads(self, x):\n",
        "        bsz = x.shape[0]\n",
        "        split_x = tf.reshape(x, (bsz, -1, self.num_heads, self.depth))\n",
        "        split_x = tf.transpose(split_x, perm=[0, 2, 1, 3])\n",
        "\n",
        "        return split_x\n",
        "\n",
        "    def combine_heads(self, x):\n",
        "        bsz = x.shape[0]\n",
        "        combined_x = tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "        combined_x = tf.reshape(combined_x, (bsz, -1, self.d_model))\n",
        "\n",
        "        return combined_x\n",
        "\n",
        "    \n",
        "    def call(self, Q, K, V, mask):\n",
        "        WQ = self.W_q(Q)\n",
        "        WK = self.W_k(K)\n",
        "        WV = self.W_v(V)\n",
        "        \n",
        "        WQ_splits = self.split_heads(WQ)\n",
        "        WK_splits = self.split_heads(WK)\n",
        "        WV_splits = self.split_heads(WV)\n",
        "        \n",
        "        out, attention_weights = self.scaled_dot_product_attention(\n",
        "            WQ_splits, WK_splits, WV_splits, mask)\n",
        "                        \n",
        "        out = self.combine_heads(out)\n",
        "        out = self.linear(out)\n",
        "            \n",
        "        return out, attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71179815",
      "metadata": {
        "id": "71179815",
        "outputId": "ac9449f4-646e-4f5a-9f20-f85d4342786a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "슝=3\n"
          ]
        }
      ],
      "source": [
        "# Position-wise Feed Forward Network 구현\n",
        "class PoswiseFeedForwardNet(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super(PoswiseFeedForwardNet, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.d_ff = d_ff\n",
        "\n",
        "        self.fc1 = tf.keras.layers.Dense(d_ff, activation='relu')\n",
        "        self.fc2 = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "    def call(self, x):\n",
        "        out = self.fc1(x)\n",
        "        out = self.fc2(out)\n",
        "        return out\n",
        "print(\"슝=3\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6700a1b",
      "metadata": {
        "id": "c6700a1b",
        "outputId": "debb8958-c52e-405b-ec6e-c2281b197be7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "슝=3\n"
          ]
        }
      ],
      "source": [
        "# Encoder의 레이어 구현\n",
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, n_heads, d_ff, dropout):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "\n",
        "        self.enc_self_attn = MultiHeadAttention(d_model, n_heads)\n",
        "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
        "\n",
        "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.do = tf.keras.layers.Dropout(dropout)\n",
        "        \n",
        "    def call(self, x, mask):\n",
        "        '''\n",
        "        Multi-Head Attention\n",
        "        '''\n",
        "        residual = x\n",
        "        out = self.norm_1(x)\n",
        "        out, enc_attn = self.enc_self_attn(out, out, out, mask)\n",
        "        out = self.do(out)\n",
        "        out += residual\n",
        "        \n",
        "        '''\n",
        "        Position-Wise Feed Forward Network\n",
        "        '''\n",
        "        residual = out\n",
        "        out = self.norm_2(out)\n",
        "        out = self.ffn(out)\n",
        "        out = self.do(out)\n",
        "        out += residual\n",
        "        \n",
        "        return out, enc_attn\n",
        "\n",
        "print(\"슝=3\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7dfce940",
      "metadata": {
        "id": "7dfce940",
        "outputId": "b96e594a-66ec-4a01-ea5c-672ccdb42236"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "슝=3\n"
          ]
        }
      ],
      "source": [
        "# Decoder 레이어 구현\n",
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "\n",
        "        self.dec_self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.enc_dec_attn = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
        "\n",
        "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.norm_3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.do = tf.keras.layers.Dropout(dropout)\n",
        "    \n",
        "    def call(self, x, enc_out, dec_enc_mask, padding_mask):\n",
        "\n",
        "        '''\n",
        "        Masked Multi-Head Attention\n",
        "        '''\n",
        "        residual = x\n",
        "        out = self.norm_1(x)\n",
        "        out, dec_attn = self.dec_self_attn(out, out, out, padding_mask)\n",
        "        out = self.do(out)\n",
        "        out += residual\n",
        "\n",
        "\n",
        "        '''\n",
        "        Multi-Head Attention\n",
        "        '''\n",
        "        residual = out\n",
        "        out = self.norm_2(out)\n",
        "        # Q, K, V 순서에 주의하세요!\n",
        "        out, dec_enc_attn = self.enc_dec_attn(Q=out, K=enc_out, V=enc_out, mask=dec_enc_mask)\n",
        "        out = self.do(out)\n",
        "        out += residual\n",
        "        \n",
        "        '''\n",
        "        Position-Wise Feed Forward Network\n",
        "        '''\n",
        "        residual = out\n",
        "        out = self.norm_3(out)\n",
        "        out = self.ffn(out)\n",
        "        out = self.do(out)\n",
        "        out += residual\n",
        "\n",
        "\n",
        "        return out, dec_attn, dec_enc_attn\n",
        "print(\"슝=3\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "999922c3",
      "metadata": {
        "id": "999922c3",
        "outputId": "3ebd934e-ca71-43e2-f80c-b2aa7f39c028"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "슝=3\n"
          ]
        }
      ],
      "source": [
        "# Encoder 구현\n",
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self,\n",
        "                    n_layers,\n",
        "                    d_model,\n",
        "                    n_heads,\n",
        "                    d_ff,\n",
        "                    dropout):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.n_layers = n_layers\n",
        "        self.enc_layers = [EncoderLayer(d_model, n_heads, d_ff, dropout) \n",
        "                        for _ in range(n_layers)]\n",
        "    \n",
        "        self.do = tf.keras.layers.Dropout(dropout)\n",
        "        \n",
        "    def call(self, x, mask):\n",
        "        out = x\n",
        "    \n",
        "        enc_attns = list()\n",
        "        for i in range(self.n_layers):\n",
        "            out, enc_attn = self.enc_layers[i](out, mask)\n",
        "            enc_attns.append(enc_attn)\n",
        "        return out, enc_attns\n",
        "print(\"슝=3\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "071be08b",
      "metadata": {
        "id": "071be08b",
        "outputId": "4a67b141-8a9a-4c56-94d5-c0c5216a28df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "슝=3\n"
          ]
        }
      ],
      "source": [
        "# Decoder 구현\n",
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self,\n",
        "                    n_layers,\n",
        "                    d_model,\n",
        "                    n_heads,\n",
        "                    d_ff,\n",
        "                    dropout):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.n_layers = n_layers\n",
        "        self.dec_layers = [DecoderLayer(d_model, n_heads, d_ff, dropout) \n",
        "                            for _ in range(n_layers)]\n",
        "                            \n",
        "                            \n",
        "    def call(self, x, enc_out, dec_enc_mask, padding_mask):\n",
        "        out = x\n",
        "    \n",
        "        dec_attns = list()\n",
        "        dec_enc_attns = list()\n",
        "        for i in range(self.n_layers):\n",
        "            out, dec_attn, dec_enc_attn = \\\n",
        "            self.dec_layers[i](out, enc_out, dec_enc_mask, padding_mask)\n",
        "\n",
        "            dec_attns.append(dec_attn)\n",
        "            dec_enc_attns.append(dec_enc_attn)\n",
        "        return out, dec_attns, dec_enc_attns\n",
        "print(\"슝=3\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a886d03b",
      "metadata": {
        "id": "a886d03b",
        "outputId": "222001ca-4515-41d5-a203-a7c7579c5911"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "슝=3\n"
          ]
        }
      ],
      "source": [
        "#트렌스포머 전체 모델 조립\n",
        "\n",
        "class Transformer(tf.keras.Model):\n",
        "    def __init__(self,\n",
        "                    n_layers,\n",
        "                    d_model,\n",
        "                    n_heads,\n",
        "                    d_ff,\n",
        "                    src_vocab_size,\n",
        "                    tgt_vocab_size,\n",
        "                    pos_len,\n",
        "                    dropout=0.2,\n",
        "                    shared_fc=True,\n",
        "                    shared_emb=False):\n",
        "        super(Transformer, self).__init__()\n",
        "        \n",
        "        self.d_model = tf.cast(d_model, tf.float32)\n",
        "\n",
        "        if shared_emb:\n",
        "            self.enc_emb = self.dec_emb = \\\n",
        "            tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
        "        else:\n",
        "            self.enc_emb = tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
        "            self.dec_emb = tf.keras.layers.Embedding(tgt_vocab_size, d_model)\n",
        "\n",
        "        self.pos_encoding = positional_encoding(pos_len, d_model)\n",
        "        self.do = tf.keras.layers.Dropout(dropout)\n",
        "\n",
        "        self.encoder = Encoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
        "        self.decoder = Decoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
        "\n",
        "        self.fc = tf.keras.layers.Dense(tgt_vocab_size)\n",
        "\n",
        "        self.shared_fc = shared_fc\n",
        "\n",
        "        if shared_fc:\n",
        "            self.fc.set_weights(tf.transpose(self.dec_emb.weights))\n",
        "\n",
        "    def embedding(self, emb, x):\n",
        "        seq_len = x.shape[1]\n",
        "\n",
        "        out = emb(x)\n",
        "\n",
        "        if self.shared_fc: out *= tf.math.sqrt(self.d_model)\n",
        "\n",
        "        out += self.pos_encoding[np.newaxis, ...][:, :seq_len, :]\n",
        "        out = self.do(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "    def call(self, enc_in, dec_in, enc_mask, dec_enc_mask, dec_mask):\n",
        "        enc_in = self.embedding(self.enc_emb, enc_in)\n",
        "        dec_in = self.embedding(self.dec_emb, dec_in)\n",
        "\n",
        "        enc_out, enc_attns = self.encoder(enc_in, enc_mask)\n",
        "        \n",
        "        dec_out, dec_attns, dec_enc_attns = \\\n",
        "        self.decoder(dec_in, enc_out, dec_enc_mask, dec_mask)\n",
        "        \n",
        "        logits = self.fc(dec_out)\n",
        "        \n",
        "        return logits, enc_attns, dec_attns, dec_enc_attns\n",
        "print(\"슝=3\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "555442a9",
      "metadata": {
        "id": "555442a9",
        "outputId": "ee328e68-0aa3-4df2-97d6-351204d5b2b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "슝=3\n"
          ]
        }
      ],
      "source": [
        "# VOCAB_SIZE \n",
        "VOCAB_SIZE = len(tokenizer.index_word)\n",
        "\n",
        "\n",
        "transformer = Transformer(\n",
        "    n_layers=2,\n",
        "    d_model=512,\n",
        "    n_heads=8,\n",
        "    d_ff=2048,\n",
        "    src_vocab_size=VOCAB_SIZE,\n",
        "    tgt_vocab_size=VOCAB_SIZE,\n",
        "    pos_len=200,\n",
        "    dropout=0.3,\n",
        "    shared_fc=True,\n",
        "    shared_emb=True)\n",
        "\n",
        "d_model = 512\n",
        "\n",
        "print(\"슝=3\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "846941e9",
      "metadata": {
        "id": "846941e9",
        "outputId": "bc78eafd-0648-4c8b-8f65-9d0c43cdd98a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "슝=3\n"
          ]
        }
      ],
      "source": [
        "# Learning Rate Scheduler 구현\n",
        "class LearningRateScheduler(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(self, d_model, warmup_steps=4000):\n",
        "        super(LearningRateScheduler, self).__init__()\n",
        "        \n",
        "        self.d_model = d_model\n",
        "        self.warmup_steps = warmup_steps\n",
        "    \n",
        "    def __call__(self, step):\n",
        "        arg1 = step ** -0.5\n",
        "        arg2 = step * (self.warmup_steps ** -1.5)\n",
        "        \n",
        "        return (self.d_model ** -0.5) * tf.math.minimum(arg1, arg2)\n",
        "print(\"슝=3\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e14423b0",
      "metadata": {
        "id": "e14423b0"
      },
      "outputs": [],
      "source": [
        "#Learning Rate & Optimizer\n",
        "learning_rate = LearningRateScheduler(d_model)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate,\n",
        "                                        beta_1=0.9,\n",
        "                                        beta_2=0.98, \n",
        "                                        epsilon=1e-9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2f4d785",
      "metadata": {
        "id": "f2f4d785",
        "outputId": "d83aa3cf-c730-4439-daad-44a1d9ae570f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "슝=3\n"
          ]
        }
      ],
      "source": [
        "# Loss Function 정의\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "\n",
        "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
        "print(\"슝=3\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8f90888",
      "metadata": {
        "id": "a8f90888",
        "outputId": "36b1f4dd-a9ca-404f-c4b8-ad9a700eb2ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "슝=3\n"
          ]
        }
      ],
      "source": [
        "# Train Step 정의\n",
        "\n",
        "@tf.function()\n",
        "def train_step(src, tgt, model, optimizer):\n",
        "    tgt_in = tgt[:, :-1]  # Decoder의 input\n",
        "    gold = tgt[:, 1:]     # Decoder의 output과 비교하기 위해 right shift를 통해 생성한 최종 타겟\n",
        "\n",
        "    enc_mask, dec_enc_mask, dec_mask = generate_masks(src, tgt_in)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions, enc_attns, dec_attns, dec_enc_attns = \\\n",
        "        model(src, tgt_in, enc_mask, dec_enc_mask, dec_mask)\n",
        "        loss = loss_function(gold, predictions)\n",
        "\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)    \n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "    return loss, enc_attns, dec_attns, dec_enc_attns\n",
        "print(\"슝=3\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de4b0d1b",
      "metadata": {
        "scrolled": true,
        "colab": {
          "referenced_widgets": [
            "7e914da42e6e430184c8d78f31a9b3db",
            "008e0852f4d44088b475273d6b73e695",
            "b6a0213fccbb4e91aa2368305c290c05",
            "df2c1be8793b4dd4a84fc2fa61679d45",
            "95251b0f567047b3812ea8a42b17885c"
          ]
        },
        "id": "de4b0d1b",
        "outputId": "77413445-080e-442d-bdb2-a053e8206cee"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7e914da42e6e430184c8d78f31a9b3db",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/478 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "008e0852f4d44088b475273d6b73e695",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/478 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b6a0213fccbb4e91aa2368305c290c05",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/478 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "df2c1be8793b4dd4a84fc2fa61679d45",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/478 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "95251b0f567047b3812ea8a42b17885c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/478 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# 훈련시키기\n",
        "\n",
        "\n",
        "EPOCHS = 5\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    total_loss = 0\n",
        "    \n",
        "    dataset_count = tf.data.experimental.cardinality(train_dataset).numpy()\n",
        "    tqdm_bar = tqdm(total=dataset_count)\n",
        "    for step, (enc_batch, dec_batch) in enumerate(train_dataset):\n",
        "        batch_loss, enc_attns, dec_attns, dec_enc_attns = \\\n",
        "        train_step(enc_batch,\n",
        "                    dec_batch,\n",
        "                    transformer,\n",
        "                    optimizer)\n",
        "\n",
        "        total_loss += batch_loss\n",
        "        \n",
        "        tqdm_bar.set_description_str('Epoch %2d' % (epoch + 1))\n",
        "        tqdm_bar.set_postfix_str('Loss %.4f' % (total_loss.numpy() / (step + 1)))\n",
        "        tqdm_bar.update()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69f58601",
      "metadata": {
        "id": "69f58601",
        "outputId": "7ad9c1bf-75c3-4541-9c74-5bbf2810f16e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "슝=3\n"
          ]
        }
      ],
      "source": [
        "def translate(tokens, model, src_tokenizer, tgt_tokenizer):\n",
        "    padded_tokens = tf.keras.preprocessing.sequence.pad_sequences([tokens],\n",
        "                                                           maxlen=50,\n",
        "                                                           padding='post')\n",
        "    ids = []\n",
        "    output = tf.expand_dims([tgt_tokenizer.bos_id()], 0)   \n",
        "    for i in range(MAX_LEN):\n",
        "        enc_padding_mask, combined_mask, dec_padding_mask = \\\n",
        "        generate_masks(padded_tokens, output)\n",
        "\n",
        "        predictions, _, _, _ = model(padded_tokens, \n",
        "                                      output,\n",
        "                                      enc_padding_mask,\n",
        "                                      combined_mask,\n",
        "                                      dec_padding_mask)\n",
        "\n",
        "        predicted_id = \\\n",
        "        tf.argmax(tf.math.softmax(predictions, axis=-1)[0, -1]).numpy().item()\n",
        "\n",
        "        if tgt_tokenizer.eos_id() == predicted_id:\n",
        "            result = tgt_tokenizer.decode_ids(ids)  \n",
        "            return result\n",
        "\n",
        "        ids.append(predicted_id)\n",
        "        output = tf.concat([output, tf.expand_dims([predicted_id], 0)], axis=-1)\n",
        "\n",
        "    result = tgt_tokenizer.decode_ids(ids)  \n",
        "    return result\n",
        "\n",
        "print(\"슝=3\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93c87ac7",
      "metadata": {
        "id": "93c87ac7"
      },
      "source": [
        "#### Step 7. 번역 성능 측정하기 BLEU Score "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7d06a72",
      "metadata": {
        "id": "f7d06a72",
        "outputId": "42401673-570b-49c0-df9d-afae4c9c6e66"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "quesions 1: 짝사랑 너무 힘들어요\n",
            "response 1: ['<start> 짝사랑 은 짝사랑 짝사랑 아서 짝사랑 인 것 . <end>', '<start> 짝사랑 은 짝사랑 은 아서 짝사랑 인 것 . <end>', '<start> 짝사랑 은 짝사랑 짝사랑 짝사랑 짝사랑 인 것 . <end>']\n",
            "\n",
            "quesions 2: 배고파서 힘들다\n",
            "response 2: ['<start> 맛난 드세요 는데 <end>', '<start> 맛난 드세요 . <end>', '<start> 얼른 드세요 는데 <end>']\n",
            "\n",
            "quesions 3: 나랑 같이 놀자\n",
            "response 3: ['<start> 저 말씀 해 보 세요 . <end>', '<start> 당신 말씀 해 보 세요 . <end>', '<start> 저 도 해 보 세요 . <end>']\n",
            "\n",
            "quesions 4: 여행가고 싶어요\n",
            "response 4: ['<start> 가 는 여행 이 가 좋 은 여행 이 가 떠나 가 는 여행 가 좋 을 거 예요 . <end>', '<start> 가 는 여행 이 가 는 은 여행 이 가 떠나 가 는 여행 가 좋 을 거 예요 . <end>', '<start> 가 는 여행 이 가 좋 을 여행 이 가 떠나 가 는 여행 가 좋 을 거 예요 . <end>']\n",
            "\n",
            "quesions 5: 스트레스 받고 있어요\n",
            "response 5: ['<start> 운동 을 해 보 세요 . <end>', '<start> 운동 을 해 살펴보 세요 . <end>', '<start> 운동 을 해 보 세요 는데 <end>']\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def beam_search_decoder(sentence, \n",
        "                        src_len,\n",
        "                        tgt_len,\n",
        "                        model,\n",
        "                        src_tokenizer,\n",
        "                        tgt_tokenizer,\n",
        "                        beam_size):\n",
        "    \n",
        "    sentence = mecab.morphs(sentence)\n",
        "    \n",
        "    def calc_prob(src_ids, tgt_ids, model):\n",
        "        enc_padding_mask, combined_mask, dec_padding_mask = \\\n",
        "        generate_masks(src_ids, tgt_ids)\n",
        "\n",
        "        predictions, enc_attns, dec_attns, dec_enc_attns =\\\n",
        "        model(src_ids, \n",
        "                tgt_ids,\n",
        "                enc_padding_mask,\n",
        "                combined_mask,\n",
        "                dec_padding_mask)\n",
        "\n",
        "        return tf.math.softmax(predictions, axis=-1)\n",
        "    \n",
        "    tokens = src_tokenizer.texts_to_sequences(sentence)\n",
        "    \n",
        "    src_in = tf.keras.preprocessing.sequence.pad_sequences(tokens,\n",
        "                                                            maxlen=src_len,\n",
        "                                                            padding='post')\n",
        "\n",
        "    pred_cache = np.zeros((beam_size * beam_size, tgt_len), dtype=np.int64)\n",
        "    pred_tmp = np.zeros((beam_size, tgt_len), dtype=np.int64)\n",
        "\n",
        "    eos_flag = np.zeros((beam_size, ), dtype=np.int64)\n",
        "    scores = np.ones((beam_size, ))\n",
        "\n",
        "    pred_tmp[:, 0] = tgt_tokenizer.word_index['<start>']\n",
        "\n",
        "    dec_in = tf.expand_dims(pred_tmp[0, :1], 0)\n",
        "    prob = calc_prob(src_in, dec_in, model)[0, -1].numpy()\n",
        "\n",
        "    for seq_pos in range(1, tgt_len):\n",
        "        score_cache = np.ones((beam_size * beam_size, ))\n",
        "\n",
        "        # init\n",
        "        for branch_idx in range(beam_size):\n",
        "            cache_pos = branch_idx*beam_size\n",
        "\n",
        "            score_cache[cache_pos:cache_pos+beam_size] = scores[branch_idx]\n",
        "            pred_cache[cache_pos:cache_pos+beam_size, :seq_pos] = \\\n",
        "            pred_tmp[branch_idx, :seq_pos]\n",
        "\n",
        "        for branch_idx in range(beam_size):\n",
        "            cache_pos = branch_idx*beam_size\n",
        "\n",
        "            if seq_pos != 1:   # 모든 Branch를 로 시작하는 경우를 방지\n",
        "                dec_in = pred_cache[branch_idx, :seq_pos]\n",
        "                dec_in = tf.expand_dims(dec_in, 0)\n",
        "\n",
        "                prob = calc_prob(src_in, dec_in, model)[0, -1].numpy()\n",
        "\n",
        "            for beam_idx in range(beam_size):\n",
        "                max_idx = np.argmax(prob)\n",
        "\n",
        "                score_cache[cache_pos+beam_idx] *= prob[max_idx]\n",
        "                pred_cache[cache_pos+beam_idx, seq_pos] = max_idx\n",
        "\n",
        "                prob[max_idx] = -1\n",
        "\n",
        "        for beam_idx in range(beam_size):\n",
        "            if eos_flag[beam_idx] == -1: continue\n",
        "\n",
        "            max_idx = np.argmax(score_cache)\n",
        "            prediction = pred_cache[max_idx, :seq_pos+1]\n",
        "\n",
        "            pred_tmp[beam_idx, :seq_pos+1] = prediction\n",
        "            scores[beam_idx] = score_cache[max_idx]\n",
        "            score_cache[max_idx] = -1\n",
        "\n",
        "            if prediction[-1] == tgt_tokenizer.word_index['<end>']:\n",
        "                eos_flag[beam_idx] = -1\n",
        "\n",
        "    pred = []\n",
        "    for long_pred in pred_tmp:\n",
        "        zero_idx = long_pred.tolist().index(tgt_tokenizer.word_index['<end>'])\n",
        "        short_pred = long_pred[:zero_idx+1]\n",
        "        pred.append(short_pred)\n",
        "        \n",
        "    return  [' '.join([src_tokenizer.index_word[j] for j in i]) for i in pred]\n",
        "\n",
        "\n",
        "MAX_LEN= 50\n",
        "\n",
        "for i, example in enumerate(examples) :\n",
        "    print(f'quesions {i+1}:',example)\n",
        "    print(f'response {i+1}:', beam_search_decoder(example,\n",
        "                        MAX_LEN,\n",
        "                        MAX_LEN,\n",
        "                        transformer,\n",
        "                        tokenizer,\n",
        "                        tokenizer,\n",
        "                        beam_size=3))\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b6a3d9d",
      "metadata": {
        "id": "2b6a3d9d",
        "outputId": "dd055a51-0059-4dd3-df1f-2c041ece8e97"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "슝=3\n"
          ]
        }
      ],
      "source": [
        "def eval_bleu_single(model, src_sentence, tgt_sentence, src_tokenizer, tgt_tokenizer, verbose=True):\n",
        "    src_tokens = tokenizer.texts_to_sequences(src_sentence)\n",
        "    tgt_tokens = tokenizer.texts_to_sequences(tgt_sentence)\n",
        "\n",
        "    if (len(src_tokens) > MAX_LEN): return None\n",
        "    if (len(tgt_tokens) > MAX_LEN): return None\n",
        "\n",
        "    reference = tgt_sentence.split()[1:-1]\n",
        "    candidate = translate(src_sentence, model, src_tokenizer, tgt_tokenizer).split()\n",
        "\n",
        "    score = sentence_bleu(reference, candidate,\n",
        "                          smoothing_function=SmoothingFunction().method1)\n",
        "\n",
        "    if verbose:\n",
        "        print(\"Source Sentence: \", src_sentence)\n",
        "        print(\"Model Prediction: \", candidate)\n",
        "        print(\"Real: \", reference)\n",
        "        print(\"Score: %lf\\n\" % score)\n",
        "        \n",
        "    return score\n",
        "        \n",
        "print('슝=3')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35470d27",
      "metadata": {
        "id": "35470d27",
        "outputId": "52986ac2-b0e1-45f4-e5a2-2bf2bae9c032"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "슝=3\n"
          ]
        }
      ],
      "source": [
        "def eval_bleu(model, src_sentences, tgt_sentence, src_tokenizer, tgt_tokenizer, verbose=True):\n",
        "    total_score = 0.0\n",
        "    sample_size = len(src_sentences)\n",
        "    \n",
        "    for idx in tqdm(range(sample_size)):\n",
        "        score = eval_bleu_single(model, src_sentences[idx], tgt_sentence[idx], src_tokenizer, tgt_tokenizer, verbose)\n",
        "        if not score: continue\n",
        "        \n",
        "        total_score += score\n",
        "    \n",
        "    print(\"Num of Sample:\", sample_size)\n",
        "    print(\"Total Score:\", total_score / sample_size)\n",
        "    \n",
        "print(\"슝=3\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6056a251",
      "metadata": {
        "id": "6056a251"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}